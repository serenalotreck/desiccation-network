{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network descriptive statistics\n",
    "In this notebook, I'll explore descriptive statistics and analyses about my citation network(s). This will include things like topic modelling in addition to more simple statistics. For the moment, I'm going to work with the Semantic Scholar dataset, so statistics about citations should be taken with a grain of salt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import requests\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import API key. This must be requested from https://www.semanticscholar.org/product/api#api-key; we save ours in an untracked file in data and import here\n",
    "import sys\n",
    "sys.path.append('../data/')\n",
    "from semantic_scholar_API_key import API_KEY\n",
    "header = {'x-api-key': API_KEY}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../data/semantic_scholar/desiccation_tolerance_10000_with_reference_abstracts_19Sep2023.jsonl') as reader:\n",
    "    papers = []\n",
    "    for obj in reader:\n",
    "        papers.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangling into a flattened object with paperID as indexer to eliminate redundant papers for analyses where we don't\n",
    "# care about the connectivity of the network\n",
    "flattened_papers = {}\n",
    "for p in papers:\n",
    "    try:\n",
    "        flattened_papers[p['paperId']] = {'title': p['title'], 'abstract': p['abstract']}\n",
    "    except KeyError:\n",
    "        flattened_papers[p['paperId']] = {'title': p['title']}\n",
    "    for r in p['references']:\n",
    "        try:\n",
    "            flattened_papers[r['paperId']] = {'title': r['title'], 'abstract': r['abstract']}\n",
    "        except KeyError:\n",
    "            flattened_papers[r['paperId']] = {'title': r['title']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using old classification from before generic bugfix for the moment\n",
    "classified = nx.read_graphml('../data/citation_network/full_10000_with_classification_gen_terms_debug_15Nov2023.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_classifications = {k: v['study_system'] for k, v in classified.nodes(data=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple descriptive statistics\n",
    "### Number of papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(flattened_papers)} unique papers in the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the overlap between both sets\n",
    "len(set(paper_classifications)), len(set(flattened_papers.keys()).intersection(set(paper_classifications)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of papers per year\n",
    "I didn't grab the years in my initial retrieval; will do this now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = len(flattened_papers)//500 + 1\n",
    "num_batches, num_batches*500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_retrieve = list(flattened_papers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "papers_with_years = []\n",
    "for i in range(num_batches):\n",
    "    ids = to_retrieve[i*500:(i+1)*500]\n",
    "    succeeded = False\n",
    "    while not succeeded:\n",
    "        r = requests.post(\n",
    "            'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "            params={'fields': 'year'},\n",
    "            json={\"ids\": ids},\n",
    "            headers=header\n",
    "        ).json()\n",
    "        if type(r) == list:\n",
    "            succeeded = True\n",
    "        else:\n",
    "            print(f'Request number {i} failed, trying again')\n",
    "    papers_with_years.extend(r)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(papers_with_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([p for p in papers_with_years if p is None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_years = [p['year'] for p in papers_with_years if (p is not None) and (p['year'] is not None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paper_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(paper_years, bins=100)\n",
    "plt.title('New publications per year for search term \"desiccation tolerance\"')\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the field of desiccation tolerance research really started to take off around 1950. What does the total publications over time look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_per_year = Counter(paper_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_years = sorted(counts_per_year.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_years = {y:(ordered_years[i] + sum(ordered_years[:i]))/1000 for i, y in enumerate(ordered_years)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cumulative_years.keys(), cumulative_years.values())\n",
    "plt.title('Cumulative publications over time for search term \"desiccation tolerance\"')\n",
    "plt.ylabel('Total publications (thousands)')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this look like if we subset by the classification that we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(paper_classifications.keys()).intersection(set([p['paperId'] for p in papers_with_years if p is not None])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are papers in the year dataset that don't match up with any `paperId` in the classification set. I believe this is due to [paper merging](https://github.com/allenai/s2-folks/issues/157); however, I am unaware of a way to get back the paperId's that I started with. For now, I'm going to move on with intersection, with the awareness that ~6,000 papers here are missing ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the classifications to the dataset\n",
    "for p in papers_with_years:\n",
    "    if p is not None:\n",
    "        try:\n",
    "            p['classification'] = paper_classifications[p['paperId']]\n",
    "        except KeyError:\n",
    "#             p['classification'] = 'missing_in_new'\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate by classification\n",
    "years_per_class = defaultdict(list)\n",
    "for p in papers_with_years:\n",
    "    if p is not None:\n",
    "        if p['year'] is not None:\n",
    "            try:\n",
    "                years_per_class[p['classification']].append(p['year'])\n",
    "            except KeyError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_per_class.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look both at the normal and cumulative versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'Plant': '#E69F00', 'Animal': '#56B4E9', 'Microbe': '#009E73', 'Fungi': '#F0E442', 'NOCLASS': '#CC79A7', 'missing_in_new': '#C7C7C7'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "for cls, yrs in years_per_class.items():\n",
    "    _ = ax.hist(yrs, bins=100, color=colors[cls], label=cls, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title('New publications per year by study system for search term \"desiccation tolerance\"')\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions look the same, which is great! Now let's look at the cumulative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(24,6))\n",
    "for cls, yrs in years_per_class.items():\n",
    "    yr_counts = Counter(yrs)\n",
    "    ordered_yrs = sorted(yr_counts.keys())\n",
    "    cumulative_years = {y:(ordered_yrs[i] + sum(ordered_yrs[:i]))/1000 for i, y in enumerate(ordered_yrs)}\n",
    "    ax.scatter(cumulative_years.keys(), cumulative_years.values(), color=colors[cls], alpha=0.5, label=cls)\n",
    "plt.legend()\n",
    "plt.title('Cumulative publications over time for search term \"desiccation tolerance\"')\n",
    "plt.ylabel('Total publications (thousands)')\n",
    "plt.xlabel('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wouldn't have expected plant and animal to have such similar numbers! Again, `NOCLASS` and `missing_in_new` are accounting for such a large portion here that we have to take these with a grain of salt, but I still think this is promising."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphs",
   "language": "python",
   "name": "graphs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
