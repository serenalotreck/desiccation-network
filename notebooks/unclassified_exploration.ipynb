{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a3880f7-0e2d-41a5-ba83-ecea2ff0912c",
   "metadata": {},
   "source": [
    "# Unclassified exploration\n",
    "We'd like to figure out exactly what's going on with why so many nodes don't get a classification. Let's reverse engineer our grpah and see what species names the non-classified abstracts have in common, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1036ace5-158d-4bb7-b6bd-85ce1e820cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from math import ceil\n",
    "from taxonerd import TaxoNERD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aaad9c9-3663-456b-8e94-3142464ca5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = nx.read_graphml('../data/citation_network/des_tol_100_classified_07Sept2023.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "908c9805-6b6a-4960-9343-1c7d24ab6dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noclass_node_ids = []\n",
    "for n, attrs in graph.nodes(data=True):\n",
    "    if attrs['study_system'] == 'NOCLASS':\n",
    "        noclass_node_ids.append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c1960-7d88-4a4f-bf21-c77eaea1053f",
   "metadata": {},
   "source": [
    "I didn't save the abstracts in the graph, so we can re-request them through the semantic scholar API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c9ad4b-9f1a-4352-a4bd-ec5725f9e8b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import API key. This must be requested from https://www.semanticscholar.org/product/api#api-key; we save ours in an untracked file in data and import here\n",
    "import sys\n",
    "sys.path.append('../data/')\n",
    "from semantic_scholar_API_key import API_KEY\n",
    "header = {'x-api-key': API_KEY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1914e45e-9c9b-435e-9dff-ab9d054a0843",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:07<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "noclass_papers = {}\n",
    "lost_refs = 0\n",
    "top_num = int(ceil(len(noclass_node_ids) / 500.0)) * 500\n",
    "for i in tqdm(range(0, top_num, 500)):\n",
    "    ids = noclass_node_ids[i: i+500]\n",
    "    search = requests.post('https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "                          params={'fields': 'title,abstract'},\n",
    "                          json={'ids': ids}).json()\n",
    "    for r in search:\n",
    "        try:\n",
    "            noclass_papers[r['paperId']] = r\n",
    "        except TypeError:\n",
    "            lost_refs += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ffb79-4ade-4c8a-8557-08145af31366",
   "metadata": {},
   "source": [
    "I want to get a sense of where the problem lies. My current hypothesis is that the failure point is entity linking, that we do classify entities in these papers, but then they get lost when we try and get an NCBI Taxonomy ID for them. Let's test that hypothesis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f85d37e6-d12b-4705-b8b6-c3949d5397ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "taxonerd = TaxoNERD()\n",
    "nlp = taxonerd.load(\"en_core_eco_biobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b83b323f-9762-4e71-bb81-46ba4c1acd55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|████████████████████████████▋                                                                                                                        | 678/3516 [06:36<27:39,  1.71it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     text \u001b[38;5;241m=\u001b[39m paperdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m ent_df \u001b[38;5;241m=\u001b[39m \u001b[43mtaxonerd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_in_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ent_df\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m     not_classified\u001b[38;5;241m.\u001b[39mappend(paper)\n",
      "File \u001b[0;32m~/anaconda3/envs/graphs/lib/python3.10/site-packages/taxonerd/taxonerd.py:121\u001b[0m, in \u001b[0;36mTaxoNERD.find_in_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_in_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m--> 121\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_to_df(doc)\n",
      "File \u001b[0;32m~/anaconda3/envs/graphs/lib/python3.10/site-packages/taxonerd/taxonerd.py:133\u001b[0m, in \u001b[0;36mTaxoNERD.ner\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_entity\u001b[39m(ent, doc, text):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m text[ent\u001b[38;5;241m.\u001b[39mstart_char : ent\u001b[38;5;241m.\u001b[39mend_char]\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (ent\u001b[38;5;241m.\u001b[39mlabel_ \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLIVB\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (ent\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mkb_ents \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinker \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;66;03m# and ((ent not in doc._.abbreviations) if self.abbrev else True)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     )\n\u001b[0;32m--> 133\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m ents \u001b[38;5;241m=\u001b[39m [ent \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments \u001b[38;5;28;01mif\u001b[39;00m is_valid_entity(ent, doc, text)]\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ents \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msenten:\n",
      "File \u001b[0;32m~/anaconda3/envs/graphs/lib/python3.10/site-packages/spacy/language.py:1026\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1026\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/graphs/lib/python3.10/site-packages/spacy_transformers/pipeline_component.py:192\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipe to one document. The document is modified in place,\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03mand returned. This usually happens under the hood when the nlp object\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03mis called on a text and all components are applied to the Doc.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mDOCS: https://spacy.io/api/transformer#call\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m install_extensions()\n\u001b[0;32m--> 192\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_annotations([doc], outputs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/anaconda3/envs/graphs/lib/python3.10/site-packages/spacy_transformers/pipeline_component.py:228\u001b[0m, in \u001b[0;36mTransformer.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    226\u001b[0m     activations \u001b[38;5;241m=\u001b[39m FullTransformerBatch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(docs))\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m batch_id \u001b[38;5;241m=\u001b[39m TransformerListener\u001b[38;5;241m.\u001b[39mget_batch_id(docs)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m listener \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlisteners:\n",
      "File \u001b[0;32m~/anaconda3/envs/graphs/lib/python3.10/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/graphs/lib/python3.10/site-packages/spacy_transformers/layers/transformer_model.py:181\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogger\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mattrs:\n\u001b[1;32m    180\u001b[0m     log_batch_size(model\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogger\u001b[39m\u001b[38;5;124m\"\u001b[39m], wordpieces, is_train)\n\u001b[0;32m--> 181\u001b[0m align \u001b[38;5;241m=\u001b[39m \u001b[43mget_alignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_spans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordpieces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m wordpieces, align \u001b[38;5;241m=\u001b[39m truncate_oversize_splits(\n\u001b[1;32m    183\u001b[0m     wordpieces, align, tokenizer\u001b[38;5;241m.\u001b[39mmodel_max_length\n\u001b[1;32m    184\u001b[0m )\n\u001b[1;32m    185\u001b[0m model_output, bp_tensors \u001b[38;5;241m=\u001b[39m transformer(wordpieces, is_train)\n",
      "File \u001b[0;32m~/anaconda3/envs/graphs/lib/python3.10/site-packages/spacy_transformers/align.py:161\u001b[0m, in \u001b[0;36mget_alignment\u001b[0;34m(spans, wordpieces, special_tokens)\u001b[0m\n\u001b[1;32m    155\u001b[0m wp_toks_filtered \u001b[38;5;241m=\u001b[39m wp_toks\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# In the case that the special tokens do not appear in the text, filter\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# them out for alignment purposes so that special tokens like \"<s>\" are\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# not aligned to the character \"s\" in the text. (If the special tokens\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# appear in the text, it's not possible to distinguish them from the\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# added special tokens, so they may be aligned incorrectly.)\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m([special \u001b[38;5;129;01min\u001b[39;00m span\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m special \u001b[38;5;129;01min\u001b[39;00m special_tokens]):\n\u001b[1;32m    162\u001b[0m     wp_toks_filtered \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    163\u001b[0m         tok \u001b[38;5;28;01mif\u001b[39;00m tok \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m special_tokens \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m wp_toks\n\u001b[1;32m    164\u001b[0m     ]\n\u001b[1;32m    165\u001b[0m span2wp, wp2span \u001b[38;5;241m=\u001b[39m get_alignments(sp_toks, wp_toks_filtered)\n",
      "File \u001b[0;32m~/anaconda3/envs/graphs/lib/python3.10/site-packages/spacy_transformers/align.py:161\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m wp_toks_filtered \u001b[38;5;241m=\u001b[39m wp_toks\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# In the case that the special tokens do not appear in the text, filter\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# them out for alignment purposes so that special tokens like \"<s>\" are\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# not aligned to the character \"s\" in the text. (If the special tokens\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# appear in the text, it's not possible to distinguish them from the\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# added special tokens, so they may be aligned incorrectly.)\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m([special \u001b[38;5;129;01min\u001b[39;00m span\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m special \u001b[38;5;129;01min\u001b[39;00m special_tokens]):\n\u001b[1;32m    162\u001b[0m     wp_toks_filtered \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    163\u001b[0m         tok \u001b[38;5;28;01mif\u001b[39;00m tok \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m special_tokens \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m wp_toks\n\u001b[1;32m    164\u001b[0m     ]\n\u001b[1;32m    165\u001b[0m span2wp, wp2span \u001b[38;5;241m=\u001b[39m get_alignments(sp_toks, wp_toks_filtered)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classified = []\n",
    "not_classified = []\n",
    "for paper, paperdata in tqdm(noclass_papers.items()):\n",
    "    try:\n",
    "        text = paperdata['title'] + ' ' + paperdata['abstract']\n",
    "    except TypeError:\n",
    "        text = paperdata['title']\n",
    "    ent_df = taxonerd.find_in_text(text)\n",
    "    if ent_df.shape[0] == 0:\n",
    "        not_classified.append(paper)\n",
    "    elif ent_df.shape[0] > 0:\n",
    "        classified.append(paper)\n",
    "print(f'Of the total of {len(noclass_papers)}, {len(classified)} contained identifiable entities, while {len(not_classified)} had no entities identified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e54b8-2ce2-4b8d-a15e-8bd7c0fdd757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
